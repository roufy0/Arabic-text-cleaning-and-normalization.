{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "9-5VO_HVSFb_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Cleaning and normalization"
      ],
      "metadata": {
        "id": "r1taXw8bXJAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular expressions for cleaning\n",
        "DIACRITICS_RE = re.compile(r\"[Ø-ØšÙ‹-ÙŸÛ–-Û­]\")\n",
        "TATWEEL = \"Ù€\"\n",
        "ALLOWED_CHARS_RE = re.compile(r\"[^Ø¡-ØºÙ-ÙŠ0-9A-Za-z@ ]\")\n",
        "ARAB2WEST_DIGITS = str.maketrans(\"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"0123456789\")"
      ],
      "metadata": {
        "id": "NnTNMkE7XZD9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAW_SAMPLES = [\n",
        "    \"\"\"\n",
        "_ÙŽ Ù•ÙÙ(~Ø¹Ø°ÙŒÙ•Ù‘Ø§ÙØ±ÙŽÙ‹ Ù•Ù•Ø·ÙŽÙŒÙ‘Ø¨ÙŽÙ‹ÙŠØ©~)Â« SÙŽkÙŽÙŽÙŽÂ«lifÙŽ  Ù…Ù•.Ø¹Ù•Ù•ØªÙ•Ù…Ø¯ÙŽ)Â»Ù ê§‚\n",
        "0549837023\n",
        "\"\"\",\n",
        "    \"\"\"Ù“Ù“Ø³Ù€Ù€.Ù€ÙƒÙ€ÙÙ„Ù€.Ù€Ù€ÙŠÙ€.Ù€Ù\n",
        "Ù“Ù“ Ù…Ù€Ù€Ø¹Ù€.Ù€ØªÙ€Ù€Ù…Ø¯ ØµÙ€Ù€Ø­Ù€.Ù€ØªÙ€Ù€ÙŠ âœ…\n",
        "Ù“ØªØ§Ø±ÙŠÙ€Ø® Ù‚Ù€Ø¯ÙŠÙ€Ù… ØªØ§Ø±ÙŠÙ€Ø® Ø¬Ø¯ÙŠØ¯ âœ…\n",
        "Ù“Ø§Ø´Ù€ Ù€Ø¹Ù€Ø§Ø± Ù…Ù€Ù€ Ù€Ø±Ø§ÙÙ‚ âœ…\n",
        "Ù“ØªÙ€.Ù€Ù‚Ù€.Ù€Ø±ÙŠÙ€Ù€Ù€Ø± Ø·Ù€Ø¨Ù€ÙŠ âœ…\n",
        "Ù“Ø³Ø¹Ø± Ù…Ù…ÙŠØ²Ø²0560516730\"\"\",\n",
        "    \"\"\"â™»ï¸*Ø³Ù€â–‘Ù€ÙƒÙ€Ù€ â–‘Ù€Ù„ÙŽÙ€ ÙŠÙ€â–‘Ù€Ù€ Ù€ÙÙ€â–‘Ù€*â™»ï¸\n",
        "âœ¨ Ø¢Ø¬Ù€ Ù€â–‘Ù€Ø¢Ø²ÙØ¢ØªÙ€ Ù€â–‘Ù€ â­Ù…Ù€â–‘Ù€Ù€ Ù€Ø±ÙØ¶Ù€Ù€ â–‘Ù€ÙŠÙ€â–‘Ù€Ø©Ù‘\n",
        "âœ¨ Ø±ÙØ³Ù€â–‘Ù€â­Ù€ Ù€Ù…Ù€â–‘Ù€ÙŠÙ€â–‘ï»² Ù…Ø¹ØªÙ…Ø¯\n",
        "âœ…à¡Žâœ”ï¸ßºÜ’ßºà¡…ð«¥ßºßºÙ‰  Ù„Ø¯ÙØ¹ Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†Ø¬Ø§Ø² 0562451912\"\"\",\n",
        "    \"\"\"Ø³\n",
        "         Ùƒ\n",
        "         Ù„\n",
        "      ÙŠ\n",
        "Ù\n",
        "        Ø§\n",
        "           Ø¹\n",
        "        Ø°\n",
        "     Ø§\n",
        " Ø±\n",
        "Ø·\n",
        "     Ø¨\n",
        "     ÙŠ\n",
        "Ø©\n",
        "Ø®\n",
        "Ø§\n",
        " Øµ\"\"\",\n",
        "    \"\"\"ðŸ”´ Ù„Ù„ÙØ§Ø¦Ø¯Ø© ðŸ”´\n",
        "Ø£ÙŠ Ø·Ø§Ù„Ø¨ ÙŠØ¨Ø­Ø« Ø¹Ù† Ø´Ø±Ø­ Ù„Ø£ÙŠ Ù…Ø§Ø¯Ø© Ø·Ø¨ÙŠØ©\n",
        "Ø­Ø±ÙÙŠÙ‹Ø§ Ø£ÙŠ Ù…Ø§Ø¯Ø© Ø·Ø¨ÙŠØ© ÙˆÙŠØ¨ÙŠ Ø´Ø±Ø­ Ù„Ù„Ù…Ø§Ø¯Ø© Ø³ÙˆØ§Ø¡ ÙŠØ¨ÙŠ Ù„Ù„Ù…Ø§Ø¯Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ùˆ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø§Øª\n",
        "Ø£Ù†ØµØ­ÙƒÙ… Ø¨Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø¯ÙˆÙƒØ§Ø¯ÙŠÙ…ÙŠ\n",
        "@DOCADEMYKSA\n",
        "@DOCADEMY\"\"\",\n",
        "]"
      ],
      "metadata": {
        "id": "e86VukR_Xc5F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 1: Cleaning and normalizing Arabic text."
      ],
      "metadata": {
        "id": "E_zVqPMVXz1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFC\", text)  # Normalize Unicode\n",
        "    text = DIACRITICS_RE.sub(\"\", text)  # Remove diacritics\n",
        "    text = text.replace(TATWEEL, \"\")  # Remove Tatweel\n",
        "    text = text.translate(ARAB2WEST_DIGITS)  # Convert Arabic digits to Western\n",
        "    text = ALLOWED_CHARS_RE.sub(\" \", text)  # Remove unwanted characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "rk5NZonNXi6L"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 2: Reconstructs fragmented Arabic words."
      ],
      "metadata": {
        "id": "CvRQO3W3X8bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_words(text: str) -> str:\n",
        "    tokens = text.split()\n",
        "    reconstructed, buffer = [], []\n",
        "\n",
        "    for token in tokens:\n",
        "        if re.fullmatch(r\"[Ø¡-ÙŠ]\", token):  # If single Arabic letter\n",
        "            buffer.append(token)\n",
        "        else:\n",
        "            if buffer:\n",
        "                reconstructed.append(\"\".join(buffer))\n",
        "                buffer.clear()\n",
        "            reconstructed.append(token)\n",
        "\n",
        "    if buffer:\n",
        "        reconstructed.append(\"\".join(buffer))\n",
        "\n",
        "    return \" \".join(reconstructed)"
      ],
      "metadata": {
        "id": "0ahwstZOXmT0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 3: Processing and generating the output file"
      ],
      "metadata": {
        "id": "9IFyEnQTYKtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_messages(messages):\n",
        "    cleaned_messages = [reconstruct_words(clean_text(msg)) for msg in messages]\n",
        "    return cleaned_messages\n",
        "\n",
        "def save_output(messages, filename=\"cleaned_messages.txt\"):\n",
        "    Path(filename).write_text(\"\\n\".join(messages), encoding=\"utf-8\")\n",
        "    print(f\"âœ… Saved â†’ {Path(filename).resolve()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cleaned = process_messages(RAW_SAMPLES)\n",
        "    save_output(cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSscbVXRXp2A",
        "outputId": "98e1408e-deb2-40fc-f34c-9fc0eb2056c7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved â†’ /content/cleaned_messages.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering File"
      ],
      "metadata": {
        "id": "NbPeBMtCYY5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 1: Define the Regular expressions for feature extraction"
      ],
      "metadata": {
        "id": "cMJKHJtSZnzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHONE_NUMBER_RE = re.compile(r\"\\b\\d{9,}\\b\")\n",
        "TELEGRAM_USERNAME_RE = re.compile(r\"@\\w+\")\n",
        "\n",
        "AD_KEYWORDS = {\n",
        "    \"Ø³ÙƒÙ„ÙŠÙ\", \"Ø§Ø¹Ø°Ø§Ø±\", \"Ø¥Ø¬Ø§Ø²Ø§Øª\", \"Ù…Ø±Ø¶ÙŠØ©\", \"ØªÙ‚Ø±ÙŠØ±\", \"Ø·Ø¨ÙŠØ©\", \"Ù…Ø¹ØªÙ…Ø¯\", \"ØµØ­ØªÙ‰\",\n",
        "    \"ØªØ§Ø±ÙŠØ®\", \"Ù‚Ø¯ÙŠÙ…\", \"Ø¬Ø¯ÙŠØ¯\", \"Ø§Ø´Ø¹Ø§Ø±\", \"Ù…Ø±Ø§ÙÙ‚\", \"Ø³Ø¹Ø±\", \"Ù…Ù…ÙŠØ²\", \"Ù„Ø¯ÙØ¹\", \"Ø¨Ø¹Ø¯\", \"Ø§Ù„Ø§Ù†Ø¬Ø§Ø²\", \"ÙÙˆØ±ÙŠ\", \"Ø®Ø§Øµ\"\n",
        "}"
      ],
      "metadata": {
        "id": "jQgvs6pKY-MI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 2: Feature Extraction Functions"
      ],
      "metadata": {
        "id": "fshD6oZ7ZL1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_phone_numbers(text: str) -> list:\n",
        "    return PHONE_NUMBER_RE.findall(text)\n",
        "\n",
        "def extract_telegram_accounts(text: str) -> list:\n",
        "    return TELEGRAM_USERNAME_RE.findall(text)\n",
        "\n",
        "def count_ad_keywords(text: str) -> int:\n",
        "    return sum(text.count(keyword) for keyword in AD_KEYWORDS)\n",
        "\n",
        "def is_ad(text: str) -> bool:\n",
        "    return bool(extract_phone_numbers(text)) or bool(extract_telegram_accounts(text)) or count_ad_keywords(text) > 0"
      ],
      "metadata": {
        "id": "akA8eQlmWpPM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 3: process the list of cleaned messages and build the dataframe"
      ],
      "metadata": {
        "id": "3lKWsXbCZcbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_features(messages):\n",
        "    data = []\n",
        "\n",
        "    for msg in messages:\n",
        "        phone_numbers = extract_phone_numbers(msg)\n",
        "        telegram_accounts = extract_telegram_accounts(msg)\n",
        "        num_keywords = count_ad_keywords(msg)\n",
        "        ad_label = \"ad\" if is_ad(msg) else \"not_ad\"\n",
        "\n",
        "        data.append({\n",
        "            \"cleaned_message\": msg,\n",
        "            \"phone_numbers\": \", \".join(phone_numbers),\n",
        "            \"telegram_accounts\": \", \".join(telegram_accounts),\n",
        "            \"num_ad_keywords\": num_keywords,\n",
        "            \"is_ad\": ad_label\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "E_1aN8RoZYtO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal 4: saves the dataframe into a csv file"
      ],
      "metadata": {
        "id": "pIGT-UBPaFHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataframe(df, filename=\"processed_features.csv\"):\n",
        "    df.to_csv(filename, index=False, encoding=\"utf-8\")\n",
        "    print(f\"âœ… Saved â†’ {Path(filename).resolve()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cleaned_messages = process_messages(RAW_SAMPLES)  # Get cleaned messages\n",
        "    df = process_features(cleaned_messages)\n",
        "    save_dataframe(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW0c3pM0aCn9",
        "outputId": "df2a65b4-9e82-49a6-ca38-b8593e23132d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved â†’ /content/processed_features.csv\n"
          ]
        }
      ]
    }
  ]
}